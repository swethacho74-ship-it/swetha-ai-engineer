<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Swetha B. | Senior AI/ML Full Stack Engineer</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Swetha.B</h1>
            <p class="title">Senior AI/ML Full Stack Engineer</p>
        </div>
        <nav>
            <a href="#about">About</a>
            <a href="#skills">Skills</a>
            <a href="#projects">Projects</a>
            <a href="#experience">Experience</a>
            <a href="#contact">Contact</a>
        </nav>
    </header>

    <main class="container">
        
        <section id="about" class="section-card">
            <h2>üëã About Me</h2>
            
            <blockquote class="hero-summary">
                Over 10 years of experience designing and deploying scalable, high-performance **AI and MLOps solutions**. Expertise in Python, Generative AI (LLMs, RAG, Agentic AI), and full-stack development on **AWS and Azure**.
            </blockquote>
            
            <p>A highly experienced Senior Python Full Stack Developer with a decade-long track record in enterprise systems. Proven ability to deliver end-to-end AI initiatives, from architecting microservices and scalable APIs to implementing Generative AI solutions. Key contributions include engineering a scalable LLM inference API handling over 1,000 requests per minute, deploying a machine learning model that reduced fraudulent transactions by 40%, and optimizing a microservices system to cut API response times by 30%.</p>
        </section>

        <section id="skills" class="section-card">
            <h2>üõ†Ô∏è Technical Skills</h2>
            
            <table class="skills-table">
                <tr>
                    <th>Generative AI & LLM Stack</th>
                    <td>RAG Pipelines, LLM Orchestration, Prompt Engineering, Agentic AI, Embedding Models (OpenAI, Cohere), RLHF</td>
                </tr>
                <tr>
                    <th>ML/Deep Learning</th>
                    <td>PyTorch, TensorFlow, Scikit-Learn, XGBoost, LightGBM, Hugging Face Transformers</td>
                </tr>
                <tr>
                    <th>MLOps & Automation</th>
                    <td>AWS SageMaker, MLflow, Kubeflow, Airflow, Weights & Biases (W&B), Docker, Kubernetes, Terraform, GitHub Actions, Jenkins</td>
                </tr>
                <tr>
                    <th>Cloud Platforms</th>
                    <td>AWS (Lambda, EC2, S3, RDS, API Gateway, CloudFormation), Azure (Synapse, Data Factory), GCP (BigQuery, Vertex AI)</td>
                </tr>
                <tr>
                    <th>Data Engineering</th>
                    <td>PySpark, Kafka, Pandas, NumPy, Spark SQL, Snowflake, Redshift, PostgreSQL, MongoDB, Redis</td>
                </tr>
                <tr>
                    <th>Vector & AI Databases</th>
                    <td>FAISS, Pinecone, Weaviate, Elasticsearch</td>
                </tr>
                <tr>
                    <th>Languages & Frameworks</th>
                    <td>Python, SQL, JavaScript (ES6+), Bash, FastAPI, Flask, Django</td>
                </tr>
            </table>
        </section>

        <section id="projects" class="section-card">
            <h2>‚≠ê Featured Projects (Case Studies)</h2>

            <div class="project-card">
                <h3>1. Low-Latency LLM Inference API & RAG System</h3>
                <h4>Problem:</h4>
                <p>Scaling Generative AI applications requires a robust backend capable of serving LLMs and integrating external knowledge for factual accuracy (RAG).</p>
                <h4>Solution:</h4>
                <ul>
                    <li>Engineered and deployed a scalable, high-throughput inference API for LLMs like GPT-3 and GPT-4 using FastAPI and Docker.</li>
                    <li>Developed Retrieval-Augmented Generation (RAG) pipelines leveraging FAISS/Pinecone/Weaviate for contextual, grounded answers.</li>
                    <li>Optim
