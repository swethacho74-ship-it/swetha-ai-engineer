 <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Swetha B. | Senior AI/ML Full Stack Engineer</title>
    <link rel="stylesheet" href="styles.css">
    </head>
</head>
<body>
    <header>
        <div class="header-content">
@@ -30,7 +30,7 @@ <h2>ðŸ‘‹ About Me</h2>
                Over 10 years of experience designing and deploying scalable, high-performance **AI and MLOps solutions**. Expertise in Python, Generative AI (LLMs, RAG, Agentic AI), and full-stack development on **AWS and Azure**.
            </blockquote>

            <p>A highly experienced Senior Python Full Stack Developer with a decade-long track record in enterprise systems. Proven ability to deliver end-to-end AI initiatives, from architecting **microservices** and scalable APIs to implementing **Generative AI** solutions. Key contributions include engineering a scalable LLM inference API handling over **1,000 requests per minute**, deploying a machine learning model that **reduced fraudulent transactions by 40%**, and optimizing a microservices system to cut API response times by **30%**.</p>
            <p>A highly experienced Senior Python Full Stack Developer with a decade-long track record in enterprise systems. Proven ability to deliver end-to-end AI initiatives, from architecting microservices and scalable APIs to implementing Generative AI solutions. Key contributions include engineering a scalable LLM inference API handling over 1,000 requests per minute, deploying a machine learning model that reduced fraudulent transactions by 40%, and optimizing a microservices system to cut API response times by 30%.</p>
        </section>

        <section id="skills" class="section-card">
@@ -77,84 +77,6 @@ <h4>Problem:</h4>
                <p>Scaling Generative AI applications requires a robust backend capable of serving LLMs and integrating external knowledge for factual accuracy (RAG).</p>
                <h4>Solution:</h4>
                <ul>
                    <li>Engineered and deployed a scalable, high-throughput inference API for LLMs like GPT-3 and GPT-4 using **FastAPI and Docker**.</li>
                    <li>Developed Retrieval-Augmented Generation (RAG) pipelines leveraging **FAISS/Pinecone/Weaviate** for contextual, grounded answers.</li>
                    <li>Optimized transformer-based NLP models through quantization and pruning, improving inference performance and reducing latency by 30%.</li>
                </ul>
                <h4>Metrics/Outcome:</h4>
                <p>The API achieved a throughput of **over 1,000 requests per minute** and delivered context-aware responses.</p>
                <p class="tech-used">**Tech Stack:** FastAPI, Docker, Kubernetes, LangChain, Hugging Face Transformers, FAISS, Pinecone, Weaviate, Python.</p>
            </div>

            <div class="project-card">
                <h3>2. Real-Time Anomaly Detection Microservices</h3>
                <h4>Problem:</h4>
                <p>Legacy systems lacked the speed and scalability to detect and prevent fraudulent transactions in real-time.</p>
                <h4>Solution:</h4>
                <ul>
                    <li>Architected a microservices-based system using **Python and FastAPI**.</li>
                    <li>Built a real-time, event-driven architecture using **Kafka and WebSockets** to stream live data feeds.</li>
                    <li>Developed and deployed a machine learning model for real-time anomaly detection.</li>
                </ul>
                <h4>Metrics/Outcome:</h4>
                <p>Successfully reduced fraudulent transactions by **40%**. The microservices architecture increased system scalability by **40%** and cut API response times by **30%**.</p>
                <p class="tech-used">**Tech Stack:** Python, FastAPI, Kafka, WebSockets, ML (Scikit-Learn/XGBoost), Docker, Microservices.</p>
            </div>

            <div class="project-card">
                <h3>3. Cloud MLOps and Infrastructure as Code (IaC) Automation</h3>
                <h4>Problem:</h4>
                <p>Inconsistent and non-reproducible ML training and deployment workflows across cloud environments (AWS/Azure).</p>
                <h4>Solution:</h4>
                <ul>
                    <li>Used **Terraform and AWS CloudFormation** to provision highly available cloud resources.</li>
                    <li>Implemented **CI/CD automation** with **GitHub Actions and Jenkins** to accelerate release cycles.</li>
                    <li>Streamlined training and deployment using **AWS SageMaker, Kubeflow, and Airflow** for scheduling, with **MLflow** for experiment tracking.</li>
                    <li>Designed serverless, event-driven AI pipelines using **AWS Lambda, Step Functions, and Terraform**.</li>
                </ul>
                <h4>Metrics/Outcome:</h4>
                <p>Led the migration of an on-premise database to AWS, designing a highly available infrastructure. Established a resilient, cost-efficient, and reproducible MLOps framework.</p>
                <p class="tech-used">**Tech Stack:** Terraform, AWS (SageMaker, Lambda, Step Functions), Airflow, MLflow, Kubeflow, Docker, Kubernetes, CI/CD (GitHub Actions, Jenkins).</p>
            </div>
        </section>
        
        <section id="experience" class="section-card">
            <h2>ðŸ’¼ Professional Experience</h2>
            <ul class="job-list">
                <li>
                    <strong>Sr. Python Full Stack Developer</strong> | Kaiser Permanente â€“ Oakland, CA 
                    <span class="dates">Oct 2023 â€“ Present</span>
                </li>
                <li>
                    <strong>Sr. Python Full Stack Developer</strong> | Goldman Sachs â€“ New York, NY
                    <span class="dates">Aug 2021 â€“ Sep 2023</span>
                </li>
                <li>
                    <strong>Python Full Stack Developer</strong> | Best Buy â€“ Richfield, MN
                    <span class="dates">Jun 2020 â€“ Apr 2021</span>
                </li>
                <li>
                    <strong>Python Full Stack Developer</strong> | Ericsson â€“ Mumbai, India
                    <span class="dates">Jan 2016 â€“ Jul 2019</span>
                </li>
                <li>
                    <strong>Python Developer</strong> | Mu Sigma â€“ Bangalore, India
                    <span class="dates">Aug 2013 â€“ Dec 2015</span>
                </li>
            </ul>
        </section>

        <section id="contact" class="section-card">
            <h2>ðŸ“§ Contact</h2>
            <p>Mail: <a href="mailto:swetha.cho74@gmail.com">swetha.cho74@gmail.com</a></p>
            <p>Contact: +1 572-239-7783</p>
            <p>LinkedIn: [Your LinkedIn Profile URL] | GitHub: [Your GitHub Profile URL]</p>
        </section>

    </main>

    <footer>
        <p>&copy; 2025 Swetha.B | Built with HTML & CSS</p>
    </footer>
</body>
</html>
                    <li>Engineered and deployed a scalable, high-throughput inference API for LLMs like GPT-3 and GPT-4 using FastAPI and Docker.</li>
                    <li>Developed Retrieval-Augmented Generation (RAG) pipelines leveraging FAISS/Pinecone/Weaviate for contextual, grounded answers.</li>
                    <li>Optim
